# -*- coding: utf-8 -*-
"""project_core

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WoOILzo6ykEo0BjbnSTFWQiBB7ftGTDt
"""

import os
import sys
import torch
import psutil
import logging


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ProjectConfig:
    """
    Configuration class for the DiffuVQA project.
    Centralizes hyperparameters and system settings.
    """
    def __init__(self):
        # System Settings
        self.seed = 42
        self.num_workers = 4  # Adjust based on CPU core count
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # Model Hyperparameters
        self.image_size = 224
        self.max_text_len = 128
        self.hidden_dim = 768

        # Training Hyperparameters
        self.batch_size = 16
        self.learning_rate = 1e-4
        self.epochs = 50

        # Paths
        self.data_dir = './data'
        self.output_dir = './checkpoints'

        self._validate_paths()

    def _validate_paths(self):
        """Ensure necessary directories exist."""
        os.makedirs(self.output_dir, exist_ok=True)

def log_system_status():
    """
    Logs current system resource usage (RAM and GPU).
    Useful for monitoring resource utilization during training loops.
    """
    # System RAM usage
    process = psutil.Process(os.getpid())
    ram_usage_gb = process.memory_info().rss / (1024 ** 3)

    # GPU VRAM usage
    if torch.cuda.is_available():
        vram_allocated = torch.cuda.memory_allocated() / (1024 ** 3)
        vram_reserved = torch.cuda.memory_reserved() / (1024 ** 3)
        gpu_status = f"| GPU VRAM: {vram_allocated:.2f}GB (Allocated), {vram_reserved:.2f}GB (Reserved)"
    else:
        gpu_status = "| GPU: Not available"

    logger.info(f"System RAM: {ram_usage_gb:.2f}GB {gpu_status}")

def save_checkpoint(model, optimizer, epoch, path):
    """
    Saves model checkpoint standard format.
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(checkpoint, path)
    logger.info(f"Checkpoint saved to {path}")

def load_checkpoint(model, optimizer, path):
    """
    Loads model checkpoint if available.
    """
    if os.path.exists(path):
        checkpoint = torch.load(path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        if optimizer:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        logger.info(f"Checkpoint loaded from {path}")
        return checkpoint.get('epoch', 0)
    else:
        logger.warning(f"No checkpoint found at {path}")
        return 0